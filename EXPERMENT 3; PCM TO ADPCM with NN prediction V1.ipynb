{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de297f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# # Define the neural network architecture\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(10, input_shape=(1,), activation='relu'),\n",
    "#     tf.keras.layers.Dense(1)\n",
    "#    ])\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# ==============================================================\n",
    "# Time series prediction model  / DNN architecture function\n",
    "model = keras.Sequential([\n",
    "keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(1,)),\n",
    "    keras.layers.Dense(2, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "# ==============================================================\n",
    "\n",
    "\n",
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "# table of index\n",
    "IndexTable = [-1, -1, -1, -1, 2, 4, 6, 8, -1, -1, -1, -1, 2, 4, 6, 8]\n",
    "\n",
    "# table of  quantizer step size\n",
    "StepSizeTable = [7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 21, 23, 25, 28, 31, 34, 37, 41, 45, 50, 55, 60, 66, 73, 80, 88, 97, 107, 118, 130, 143, 157, 173, 190, 209, 230, 253, 279, 307, 337, 371, 408, 449, 494, 544, 598, 658, 724, 796, 876, 963, 1060, 1166, 1282, 1411, 1552, 1707, 1878, 2066, 2272, 2499, 2749, 3024, 3327, 3660, 4026, 4428, 4871, 5358, 5894, 6484, 7132, 7845, 8630, 9493, 10442, 11487, 12635, 13899, 15289, 16818, 18500, 20350, 22385, 24623, 27086, 29794, 32767]\n",
    "\n",
    "\n",
    "predsample = 0\n",
    "index = 0\n",
    "\n",
    "def ADPCM_Encode(sample):\n",
    "    global index\n",
    "    global predsample\n",
    "    \n",
    "    global diffq\n",
    "    global ore_diff\n",
    "    global error_signal\n",
    "    \n",
    "    code = 0\n",
    "    \n",
    "    step_size = StepSizeTable[index]\n",
    "\n",
    "    # Use the neural network to predict the difference\n",
    "    #prediction = model.predict(predsample)\n",
    "    prediction = model.predict(np.array([predsample]))\n",
    "\n",
    "    # calculate difference\n",
    "    diff = sample - predsample + prediction\n",
    "    \n",
    "    if diff < 0:\n",
    "        code = 8\n",
    "        diff = -diff\n",
    "    # quantize the diff into ADPCM code\n",
    "    # inverse quantize the code into a predicted diff\n",
    "    tmpstep = step_size\n",
    "    diffq = step_size >> 3\n",
    "    if diff >= tmpstep:\n",
    "        code = code | 0x04\n",
    "        diff -= tmpstep\n",
    "        diffq = diffq + step_size\n",
    "    tmpstep = tmpstep >> 1\n",
    "    if diff >= tmpstep:\n",
    "        code = code | 0x02\n",
    "        diff = diff - tmpstep\n",
    "        diffq = diffq + (step_size >> 1)\n",
    "    tmpstep = tmpstep >> 1\n",
    "    if diff >= tmpstep:\n",
    "        code = code | 0x01\n",
    "        diffq = diffq + (step_size >> 2)\n",
    "        \n",
    "    # fixed predictor to get new predicted sample\n",
    "    if code & 8:\n",
    "        predsample = predsample - diffq\n",
    "    else:\n",
    "        predsample = predsample + diffq\n",
    "        \n",
    "    # check for overflow\n",
    "    if predsample > 32767:\n",
    "        predsample = 32767\n",
    "    elif predsample < -32768:\n",
    "        predsample = -32768\n",
    "        \n",
    "    # find new stepsize index\n",
    "    index += IndexTable[code]\n",
    "    \n",
    "    # check for overflow\n",
    "    if index < 0:\n",
    "        index = 0\n",
    "    if index > 88:\n",
    "        index = 88\n",
    "    \n",
    "    # update the model's parameters based on the prediction error\n",
    "         \n",
    "    model.fit(np.array([predsample]), diff, epochs=1, verbose=0)\n",
    "    \n",
    "    # return new ADPCM code code & 0x0f == code and predsample\n",
    "    return code & 0x0f, predsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29c9d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%pmatplotlib inline\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "# ...............scipy wave based data..............\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "audio_file = 'audio_data/Test_dataset_All_4_speakers_merged_MS_and_FS.wav'\n",
    "fs, data = wavfile.read(audio_file)\n",
    "ore_data=data[:]\n",
    "\n",
    "# ================================================================\n",
    "org_samp=[]\n",
    "pre_samp=[]\n",
    "oreginal_diff=[]\n",
    "adpcm_code = []\n",
    "q_diff=[]\n",
    "err_signal=[]\n",
    "\n",
    "for i in range(len(ore_data)):\n",
    "    \n",
    "    ADPCM_sample,pred = ADPCM_Encode(ore_data[i]) \n",
    "    print(\"Sample:\", i)\n",
    "    #print('................................')\n",
    "    \n",
    "    org_samp.append(ore_data[i])\n",
    "    pre_samp.append(pred)\n",
    "        \n",
    "org_samp=np.array(org_samp)           #oreginal signal samples\n",
    "pre_samp=np.array(pre_samp)           #predicted sample \n",
    "\n",
    "oreginal_diff=np.array(oreginal_diff) #the oreginal and predicted sample difference\n",
    "adpcm_code = np.array(adpcm_code)     #the ADPCM code the is transmited to decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_samp[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_samp[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc095ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data Between 0 and 1\n",
    "y = org_samp\n",
    "ypn = pre_samp\n",
    "\n",
    "org_samp = np.array(y)\n",
    "# goal : range [0, 1]\n",
    "org_samp = (org_samp - min(org_samp)) / ( max(org_samp) - min(org_samp) )\n",
    "\n",
    "pre_samp = np.array(ypn)\n",
    "pre_samp = (pre_samp - min(pre_samp)) / ( max(pre_samp) - min(pre_samp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc3fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#====================This is a one step forecast (based on that we have constructed the model)====================\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(org_samp[:], label = 'Original speech signal')\n",
    "plt.plot(pre_samp[:], label = 'Predicted speech signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf256609",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_signal=org_samp-pre_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e2e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#====================kjjmmmmmmmmmmmmmmmmmm======\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Oreginal speech signal data\n",
    "plt.title('The Original PCM and predicted Speech Signal') \n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Amplitude (Quantization level)')\n",
    "plt.plot(org_samp, 'b', label='Original Speech Signal ')\n",
    "plt.plot(pre_samp, 'g', label='predicted Speech Signal')\n",
    "plt.plot(error_signal, 'r', label='Error Speech Signal')\n",
    "plt.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ee73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#====================kjjmmmmmmmmmmmmmmmmmm======\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Oreginal speech signal data\n",
    "plt.title('The Original PCM and predicted Speech Signal it was randomly taken from 3000th to 3050th') \n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Amplitude (Quantization level)')\n",
    "plt.plot(org_samp[3000:3050], 'b', label='Original Speech Signal ')\n",
    "plt.plot(pre_samp[3000:3050], 'g', label='predicted Speech Signal')\n",
    "\n",
    "plt.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('The Original PCM and predicted Speech Signal it was randomly taken from 400000th to 400050th') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17884ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#====================kjjmmmmmmmmmmmmmmmmmm======\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Oreginal speech signal data\n",
    "plt.title('The Original PCM and predicted Speech Signal it was randomly taken from 400000th to 400050th') \n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Amplitude (Quantization level)')\n",
    "plt.plot(org_samp[400000:400050], 'b', label='Original Speech Signal ')\n",
    "plt.plot(pre_samp[400000:400050], 'g', label='predicted Speech Signal')\n",
    "\n",
    "plt.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================kjjmmmmmmmmmmmmmmmmmm======\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Oreginal speech signal data\n",
    "plt.title('The Original PCM and predicted Speech Signal it was randomly taken from 700,000th to 700,050th') \n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Amplitude (Quantization level)')\n",
    "plt.plot(org_samp[700000:700050], 'b', label='Original Speech Signal ')\n",
    "plt.plot(pre_samp[700000:700050], 'g', label='predicted Speech Signal')\n",
    "\n",
    "plt.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee9a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Calculating Signal to Noise Ratio (SNR)===========\n",
    "\n",
    "error = org_samp[:] - pre_samp[:]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(error, 'b')\n",
    "plt.show()\n",
    "\n",
    "org_sigpow = 10*np.log10(np.mean(pow(np.abs(org_samp[:]),2)))\n",
    "error_mse = 10*np.log10(np.mean(pow(np.abs(error),2)))\n",
    "\n",
    "#print(dnn_mse, dnn_sigpow, lms_mse, lms_sigpow)\n",
    "print(\"NN based ADPCM predictor SNR:\", org_sigpow - error_mse, \"dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dc004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
